{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project\n",
    "## Task\n",
    "The task of this deep learning project was to create and train a neural network for number recognition of the MNIST dataset, which contains 70000 hand-written numbers from 0 to 10. The main priority is performance followed by explainability, while data analysis comes last.\n",
    "\n",
    "As a first step the given ways of tutorial 1 were tried and rated by their performance. Version 0 is without regularization, where different validation splits (15% and 20%) and early stopping have been tried. Version 1 is with L2 regularizer and version 2 with dropout function.\n",
    "\n",
    "Even though version 0 had the best performance, I decided to work on with dropout because of problems with noise of loss and overfitting. The model is a sequential model with 5 layers. The first layer is a flatten layer which flattens the input data from a 28x28 matrix to a 784 element vector. The next two layers are dense layers with 512 and 128 neurons respectively, and both use the ReLU activation function. These layers are followed by two dropout layers with a dropout rate of 0.5, which helps to prevent overfitting by randomly setting a fraction of the input units to 0 during training. The final layer is a dense layer with 10 neurons and a softmax activation function. It is compiled with the Adam optimizer, a sparse categorical crossentropy loss function and accuracy as a metric. The model is then trained for 10 epochs with a batch size of 32 and a validation split of 0.15, very similar to the test splitting of MNIST (training set: 60000, test set: 10000 -> 14%). With this model it is able to reach an accuracy of 98% and a loss of 0.07."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and normalize dataset and define plottinf function\n",
    "(x_train, y_train), (x_test, y_test)=tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test=x_train/255.0, x_test/255.0\n",
    "\n",
    "def plot (train_history):\n",
    "    plt.plot(train_history.history['loss'], label='train')\n",
    "    plt.plot(train_history.history['val_loss'], label='val')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_history.history['accuracy'], label='train')\n",
    "    plt.plot(train_history.history['val_accuracy'], label='val')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define structure of neural network with dropout\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.5),\n",
    "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.5),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training parameters\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model and give out graph of loss and accuracy\n",
    "history=model.fit(x_train, \n",
    "          y_train, \n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          shuffle=True, \n",
    "          validation_split=0.15\n",
    "         )\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate on test set\n",
    "validation_acc = model.evaluate(x_test, y_test)\n",
    "print('Validation accuracy with test set:', validation_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with convolutional layers\n",
    "To improve the performance, I tried a model with covolutional layers. The main concept of the code and the concept of convolutional layers is displayed in the picture below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"conv_layers.png\" width=\"800\"><br>\n",
    "Quelle: https://www.youtube.com/watch?v=9cPMFTwBdM4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional neural network consists of two convolutional layers, each followed by a max pooling layer, and then a fully connected dense layer.\n",
    "\n",
    "The to_categorial method labels in the y_train and y_test arrays into one-hot encoded labels. One-hot encoding is a process where a categorical label is represented as an array of binary values, where only one element is 1 and the rest are 0. This is often used in classification problems where the categorical labels are not ordinal and there is no inherent ordering between them.\n",
    "\n",
    "It is then compiled with the rmsprop optimizer, the categorical cross-entropy loss function and the accuracy metric. The training is performed for 5 epochs with a batch size of 32 and shuffling of the training data. 15% of the training data is used for validation. With this model it was able to reach an accuracy of 99% and a loss of 0.03.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to one-hot encoded labels\n",
    "from keras.utils import to_categorical\n",
    "y_train=to_categorical(y_train)\n",
    "y_test=to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define structure of neural network with convolutional and dense layers\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32,(3,3), activation=tf.nn.relu, input_shape=(28,28,1)),\n",
    "  tf.keras.layers.MaxPooling2D((2,2)),\n",
    "  tf.keras.layers.Conv2D(64,(3,3), activation=tf.nn.relu),\n",
    "  tf.keras.layers.MaxPooling2D((2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training parameters\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "history=model.fit(x_train, \n",
    "          y_train, \n",
    "          epochs=5,\n",
    "          batch_size=32, \n",
    "          shuffle=True,\n",
    "          validation_split=0.15,\n",
    "         )\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate on test set\n",
    "validation_acc = model.evaluate(x_test, y_test)\n",
    "print('Validation accuracy with test set:', validation_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f2f6c680cf484319d387fabac80ca4ff4fc33965036e02100b3fe02600f1423"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
